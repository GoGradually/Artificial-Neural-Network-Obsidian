1. **Regularization (정규화):**
    - Regularization은 모델의 복잡성을 제어하고, 오버피팅을 방지하기 위한 기술입니다.
    - 주로 모델의 가중치(Weight)를 조절하여 사용하며, L1, L2 정규화 등 다양한 형태가 있습니다.
    - L1 정규화는 가중치의 절댓값을 손실 함수에 추가하여 가중치를 작게 유지하고 특정 가중치를 0으로 만드는 경향이 있습니다.
    - L2 정규화는 가중치의 제곱을 손실 함수에 추가하여 가중치의 크기를 줄이는 역할을 합니다.
    - 목표는 모델이 복잡한 결정 경계 대신 간단한 결정 경계를 만들도록 유도하여 일반화 성능을 향상시킵니다.
2. **Normalization (정규화):**
    - Normalization은 데이터 또는 활성화 값을 정규화하여 모델의 학습을 안정화시키고 수렴 속도를 향상시키는 기술입니다.
    - Batch Normalization과 Layer Normalization과 같은 기법을 사용하여 활성화 값의 평균과 분산을 조절합니다.
    - 정규화는 모델의 학습을 도울 뿐만 아니라, 그라디언트 소실 문제를 완화시키고 빠른 수렴을 돕습니다.