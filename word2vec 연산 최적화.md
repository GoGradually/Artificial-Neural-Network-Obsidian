### solution 1. 트리구조로 softmax 계산
![[Pasted image 20231117174334.png]]
단어 C에 대한 타겟의 등장확률 $P(t|c)$ 를 트리 구조로 계산하게 한다.
$\sigma (x) = 1 - \sigma (-x)$
사실 잘 이해가 안간다. 어떤 자료구조로 트리로 만드는건지

### solution 2. negative sampling
전체 임베딩 벡터에 대하여
타겟 단어나 주변 단어인  positive vector 은 모두 업데이트하고,
타겟 단어와 거리가 먼 negative vector 은 일부만 업데이트하는 방법.
그 일부를 고르는 것을 sampling 이라고 한다.

##### 핵심 아이디어
1. **Positive Example (실제 주변 단어):**
    - 주어진 중심 단어에 대한 실제 주변 단어를 사용하여 손실을 계산합니다.
2. **Negative Examples (가짜 주변 단어):**
    - 실제 주변 단어 외에도 일부 "가짜" 또는 "부정적인" 단어 쌍을 생성합니다. 이는 실제 주변 단어가 아닌 단어들을 모델이 예측하도록 하는 것입니다.
3. **Binary Classification 문제로 변환:**
    - 중심 단어와 주변 단어, 그리고 부정적인 단어들 간의 쌍을 이진 분류 문제로 취급합니다. 모델은 주어진 쌍이 실제인지(1) 아니면 가짜인지(0) 예측하도록 학습됩니다.
4. **손실 계산:**
    - 실제 주변 단어에 대한 손실과 가짜 주변 단어에 대한 손실을 계산하고 두 손실을 합칩니다.

하나의 N-ary softmax 함수를 여러개(<N) 의 이진 분류 함수로 바꾼다
![[Pasted image 20231117190937.png]]


##### sampling 방법
-> 확률에 의한 임의 추출
$$P_n(w_i) = \left( \frac{f(w_i)}{\sum_{j = 1}^{n}f(w_j)} \right)^{\frac{3}{4}} $$
분모 = 중복을 허용한 전체 단어의 수
분자 = 해당 단어의 출현 빈도
3/4 는 [0, 1] 까지 가능한데, 경험적으로 3/4 가 가장 좋은 성능을 보인다고 한다.