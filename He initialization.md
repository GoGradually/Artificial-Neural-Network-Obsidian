![[Pasted image 20231002120427.png]]
[['~'의 의미]]
$n_{in}$ = label의 input feature의 수

Chat GPT answer of He Initialization
	He 초기화(He Initialization)는 딥 뉴럴 네트워크에서 가중치(weight)를 초기화하는 방법 중 하나로, ReLU(Rectified Linear Unit) 활성화 함수와 함께 사용될 때 주로 사용됩니다. He 초기화는 Xavier 초기화와 마찬가지로 그래디언트 소실(vanishing gradient) 문제를 줄이고 학습을 더욱 효율적으로 만드는 목적으로 개발되었습니다.
	He 초기화의 주요 아이디어는 ReLU 활성화 함수의 특성을 고려하여 가중치를 초기화하는 것입니다. ReLU 함수는 음수 입력에 대해서는 0을 출력하고 양수 입력에 대해서는 입력 값을 그대로 출력합니다. 이 때, He 초기화는 다음과 같은 방식으로 가중치를 초기화합니다:
	1. 가중치 초기화: 가중치 행렬을 초기화할 때, 평균이 0이고 분산(또는 표준편차)가 2/n을 사용하여 초기화합니다.
	2. 수식으로 표현하면, 특정 가중치 행렬 W의 초기화는 다음과 같습니다.
	    W = random values with mean=0 and variance=2/n
	    여기서 n은 이전 레이어의 입력 뉴런 수입니다.
	He 초기화는 ReLU 활성화 함수와 함께 사용될 때, 가중치를 더욱 적절하게 초기화하여 그래디언트 소실 문제를 완화하고, 네트워크의 학습 성능을 향상시킬 수 있습니다. 특히 딥 뉴럴 네트워크에서 많이 활용되며, 이 초기화 방법을 사용하면 네트워크가 더 빠르게 수렴하고 더 나은 성능을 달성할 수 있습니다.