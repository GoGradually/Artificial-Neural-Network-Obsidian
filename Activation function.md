활성화 함수(Activation Function)은 인공 신경망의 뉴런(neuron) 또는 노드(node)에서 입력 신호의 가중합을 계산한 후 출력 신호를 생성하는 함수입니다. 활성화 함수는 인공 신경망의 비선형성을 도입하고, 모델이 복잡한 함수를 근사하도록 도와줍니다. 이 함수는 신경망의 각 레이어에서 사용되며, 주로 다음과 같은 역할을 합니다:

1. **비선형성 도입**: 활성화 함수는 입력과 출력 간의 비선형 관계를 만듭니다. 이것은 신경망이 복잡한 데이터 패턴 및 관계를 학습할 수 있게 합니다. 선형 활성화 함수의 경우 여러 층의 선형 조합은 결국 하나의 선형 변환으로 간주됩니다.
   - 같이보기 : [[Vanishing Gradient]]
    
2. **신경망의 표현력 향상**: 다양한 종류의 활성화 함수를 사용하면 신경망이 다양한 함수를 근사할 수 있습니다. 예를 들어, 시그모이드(Sigmoid), 렐루(ReLU), 하이퍼볼릭 탄젠트(Tanh) 등의 활성화 함수가 널리 사용됩니다.
    

일반적으로 사용되는 몇 가지 활성화 함수에는 다음과 같은 것들이 있습니다:

- **[[시그모이드(Sigmoid) 함수]]**: S자 형태의 곡선을 가지며, 출력 범위가 0에서 1 사이입니다. 주로 이진 분류 문제의 출력 레이어에서 사용됩니다.
    
- **[[하이퍼볼릭 탄젠트(Tanh) 함수]]**: S자 형태의 곡선을 가지며, 출력 범위가 -1에서 1 사이입니다. 은닉 레이어의 활성화 함수로 사용됩니다.
    
- **[[ReLU]]**: 입력이 양수인 경우에는 입력 값을 그대로 출력하고, 음수인 경우에는 0을 출력하는 간단한 함수입니다. 주로 은닉 레이어에서 사용되며, 계산 효율성과 학습 성능에서 우수한 결과를 내는 경향이 있습니다.
    
- **[[Leakly ReLU]]**: ReLU와 유사하지만 음수 입력에 대해 작은 기울기를 갖는다는 차이가 있습니다. 이는 ReLU의 "죽은 뉴런" 문제를 해결하는 데 도움이 될 수 있습니다.
    
- **[[소프트맥스(Softmax) 함수]]**: 다중 클래스 분류 문제의 출력 레이어에서 사용되며, 각 클래스에 대한 확률 분포를 생성합니다.
    

활성화 함수는 신경망의 학습 및 예측에 중요한 역할을 하며, 올바른 활성화 함수의 선택은 모델의 성능에 큰 영향을 미칩니다.