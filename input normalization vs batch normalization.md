1. **Input Normalization (입력 정규화):**
    
    - Input normalization은 데이터의 입력 피처(특성)를 평균과 표준 편차로 조정하는 것을 의미합니다.
    - 이것은 주로 데이터 전처리의 일부로 사용되며, 학습 데이터 전체의 평균과 표준 편차를 계산하여 훈련 및 테스트 데이터에 동일하게 적용하는 경우가 많습니다. 이로써 입력 데이터의 스케일을 조정하고, 모델 학습을 안정화시키는 데 도움이 될 수 있습니다.
2. **Batch Normalization (배치 정규화):**
    
    - Batch normalization은 인공 신경망의 레이어 간의 활성화 값을 정규화하는 방법으로, 훈련 과정 중에 추가된 정규화 레이어입니다.
    - 각 레이어의 활성화 값을 평균과 분산을 사용하여 정규화하고, 이를 활성화 함수 직전에 적용합니다.
    - 배치 정규화는 레이어 간의 데이터 분포 변화를 줄이고, 학습을 안정화시키며, 더 빠른 수렴과 더 나은 일반화 성능을 제공할 수 있습니다.