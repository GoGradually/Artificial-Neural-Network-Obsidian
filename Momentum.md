[[Stochastic Gradient Descent(SGD)]] 에 "모멘텀"을 추가해주는 것.
일반적으로 Momentum이라고 불린다.
모멘텀은 기존 SGD에 [[Exponentially weighted averages]]을 추가해준다.
일반적으로 $\beta$는 0.9로 설정해준다.

장점
1. 밖으로 확 튀어나가는 상황(오버슈팅, overshooting)을 막는다!
![[Pasted image 20231003001736.png]]
2. 지역 최소값에서의 멈춤 방지 & 수렴 속도 향상 
   (Local minimum & Plateau 탈출)
![[Pasted image 20231003002343.png]]




구현 디테일 
![[Pasted image 20231003002113.png]]






모멘텀(Momentum)은 경사 하강법(Gradient Descent)의 변형 중 하나로, 기울기(gradient)의 지수 가중 이동 평균(Exponential Moving Average)을 사용하여 최적화 속도를 높이고 수렴 속도를 향상시키는 기법입니다. 모멘텀은 주로 신경망 및 딥러닝 모델의 학습에서 많이 사용됩니다.

모멘텀을 사용하는 경사 하강법은 다음과 같이 작동합니다:

1. 각 반복(iteration)에서 현재의 기울기(gradient)를 계산합니다.
    
2. 현재 기울기와 이전 반복에서의 기울기에 모멘텀 하이퍼파라미터(일반적으로 0.9로 설정)를 사용하여 가중 평균을 계산합니다.
    
3. 계산된 가중 평균을 사용하여 가중치(weight)를 업데이트합니다.
    

모멘텀을 사용하면 다음과 같은 이점이 있습니다:

1. **수렴 속도 향상**: 모멘텀을 사용하면 기울기가 0이 아닌 방향으로 계속 이동하므로, 경사 하강법보다 더 빠르게 수렴할 수 있습니다.
    
2. **지역 최소값 피해**: 모멘텀은 지역 최소값(local minima)에 빠르게 빠져나오는 데 도움을 줄 수 있습니다. 모멘텀은 이전 기울기의 방향을 기억하므로, 지역 최소값에서의 멈춤을 방지합니다.
    
3. **오버슈팅 완화**: 모멘텀은 가중치 업데이트를 부드럽게 수행하므로 큰 오버슈팅(overshooting)을 방지하고 안정된 학습 경로를 유지합니다.
    

모멘텀은 경사 하강법의 변형 중 하나로, 학습률(learning rate)과 함께 조절되어야 합니다. 일반적으로 모멘텀을 사용할 때는 학습률을 상대적으로 높게 설정하는 것이 일반적입니다. 모멘텀을 효과적으로 사용하면 신경망 학습에서 안정성과 성능을 향상시킬 수 있습니다.