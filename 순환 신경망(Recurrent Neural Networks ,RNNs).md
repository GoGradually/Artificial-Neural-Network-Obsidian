반복되는 루프같은 레이어를 가지는 신경망.
자기 자신을 입력이 끝날때까지 반복하기 때문에 가변적인 길이의 입력에도 대응이 가능하다.
hidden state 라는 개념을 이용해 정보를 유지하고 전달한다.
![[Pasted image 20231116133545.png]]
$x^{<t>}$ = t번째 입력 단어
$y^{<t>}$ = t번째 출력 단어?
모델 파라미터는 $W_{ya}, W_{ax}, W_{aa}$ 가 있다.
이 모델 파라미터 세개가 모든 시계열에 적용된다.
히든 스테이트의 활성 함수로는 [[하이퍼볼릭 탄젠트(Tanh) 함수]] 가 사용된다.

$$a^{<t>} = g_h(\bf{W}_{aa}\bf{a}^{<t-1>} + \bf{W}_{ax}\bf{x}^{<t>} + \bf{b}_a)$$
$$\hat{y}^{<t>} = g_o(\bf{W}_{ya}\bf{a}^{<t>} + \bf{b}_y)$$
### 순환 신경망의 연산 단순화
![[Pasted image 20231116134603.png]]
을 가정하면![[Pasted image 20231116134641.png]]
다음과 같이 concatenate 하여 연산이 가능하다.
; = concatenate 연산
$$a^{<t>} = g_h(\bf{W}_{a}[\bf{a}^{<t-1>};\bf{x}^{<t>}] + \bf{b}_a)$$
$$\hat{y}^{<t>} = g_o(\bf{W}_{y}\bf{a}^{<t>} + \bf{b}_y)$$

### 순환 신경망의 순전파, 역전파
![[Pasted image 20231116135033.png]]

### 순환 신경망의 손실함수
시간별 손실 함수
$$L^{<t>}(y^{<t>}, \hat y^{<t>}) = -y^{<t>}log\hat y^{<t>} - (1- y^{<t>})log(1-\hat y^{<t>})$$
통합 손실 함수
$$L(y, \hat y) = \sum_{t = 1}^{T_y}L^{<t>}(y^{<t>},\hat y^{<t>})$$
$\frac{1}{T_y}$ 로 Normalize가 가능하다.