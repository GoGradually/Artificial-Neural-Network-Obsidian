역전파(Backpropagation)는 인공 신경망의 학습 알고리즘 중 하나로, 네트워크의 오차를 역으로 전파하면서 가중치와 편향을 조정하여 모델을 학습시키는 방법입니다. 아래는 역전파의 엄밀한 정의와 동작 원리를 설명합니다:

1. **오차 역전파**: 역전파는 신경망이 예측한 출력과 실제 정답 사이의 오차를 계산합니다. 주로 평균 제곱 오차(Mean Squared Error)나 크로스 엔트로피(Cross-Entropy)와 같은 [[Loss Function]]를 사용하여 오차를 측정합니다.
    
2. **그래디언트 계산**: 계산된 오차를 사용하여 네트워크의 각 가중치(weight)와 편향(bias)에 대한 그래디언트(기울기)를 계산합니다. 이 그래디언트는 손실 함수를 각 파라미터로 미분한 값으로, 파라미터 조정 방향과 크기를 결정합니다.
    
3. **업데이트**: 계산된 그래디언트를 사용하여 네트워크의 가중치와 편향을 업데이트합니다. 일반적으로 경사 하강법([[Gradient Descent]]) 또는 그 변형 중 하나를 사용하여 파라미터를 조정합니다. 역전파를 통해 오차를 최소화하는 방향으로 파라미터를 업데이트하면 모델이 학습됩니다.
    
4. **순방향 전파**: 새로운 가중치와 편향을 사용하여 입력 데이터를 순방향으로 전파하면서 새로운 예측을 생성합니다. 이 과정을 통해 오차가 줄어들고 모델의 예측이 실제에 더 가까워집니다.
    
5. **반복**: 위의 단계를 여러 번 반복하면서 네트워크는 학습 데이터에 대해 더 나은 예측을 수행하도록 조정됩니다. 이 과정을 [[에포크(epoch)]]라고 하며, 학습 데이터를 여러 번 반복하여 모델을 훈련합니다.
    

요약하면, 역전파는 신경망이 예측한 오차를 다시 입력 방향으로 전파하여 각 파라미터에 대한 그래디언트를 계산하고, 이를 사용하여 모델을 조정하는 핵심 학습 알고리즘입니다. 이를 통해 신경망은 주어진 작업에 대해 최적의 파라미터를 학습하고 예측을 수행할 수 있게 됩니다.

다변수함수의 미분을 통해 $\nabla J_\theta(\theta)$를 구하여 그를 [[Learning Rate]] 와 곱하여 $J(\theta)$ 값에 더하는 것.
$\theta$ = [[model parameter]]