[[Pytorch 사용법]]
[[구현한 과제들]]

- [[중간고사 공부용 정리]]
Aritificial neural network

- [[Traning]]
- [[Classification vs Regression]]
- [[Evaluation of Algorithm]]
- [[Neuron]]
- [[Regression model]]
- [[Loss Function]]
- [[Linear Regression]]
- [[Logistic Regression]]
- [[Activation function]]
- [[model parameter]]
- [[Hyperparameter]]
- [[역전파(Backpropagation)]]
- [[Gradient Descent]]
- [[Shallow Neural Network(SNN)]]
- [[Deep Neural Network(DNN)]]
- [[Stochastic Gradient Descent(SGD)]]
- [[다중 분류(Multi-class Classification)]]
- [[모델의 성능을 높히는 법]] -> [[적절하게 fitting하는 법]]
- [[Bias 와 Variance]]
- [[과적합(Overfitting)]]
- [[정규화(Regularization)]]
- [[Weight Initialization]]
- [[Optimization Algorithms]]
- [[합성곱 레이어(Convolution Layer)]]
- [[합성곱 신경망(Convolutional Neural Network)]]
- [[합성곱 신경망에서 매개변수 개수 측정 방법]]
- [[배치 정규화(Batch Normalization)]]


- [[ResNet]]
- [[object detection]]
- [[sliding window method]]
- [[FC없이 합성곱만으로 모델 구현하기]]
- [[YOLO(You Only Look Once)]]
- [[Sequence Data]]
- [[순환 신경망(Recurrent Neural Networks ,RNNs)]]
- [[RNNs 의 위상]]
- [[LSTM]]
- [[RNN의 중첩 딥 러닝]]
- [[word embedding]]
- [[Word2Vec]]
- [[GloVe]]
- [[NLP 에서 가변적인 길이의 입력 데이터 처리 방법]]
- [[Fine Tuning vs Freezing]]
- [[DL15_LAB_RNN.pdf]]
- [[DL16 Sequence2Sequence.pdf]]
- [[RNN data preprocessing]]
- [[Packed Sequence]]
- [[sequence to sequence]]
- [[Attention model]]
- [[Transformers]]
- [[Beam Search]]
- [[DL16 _classroom_Sequence2Sequence.pdf]]
- [[BERT]]
--미분류 정보
레이어 수 = 모델 매개변수를 가지고 있는 층 수 -> "입력층 제외" 모든 층 수